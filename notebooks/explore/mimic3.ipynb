{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "# get data path from project directory\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import join, split\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "data_path = join(Path(os.getcwd()).parent.parent, 'data')\n",
    "mimic_path = join(data_path, 'raw', 'mimic-iii-clinical-database-1.4')\n",
    "extr_path = join(data_path, 'interim', 'mimic_iii_ml_for_health')\n",
    "processed_path = join(data_path, 'formatted', 'mimic-iii-clinical-database-1.4')\n",
    "import pyarrow as pa\n",
    "from ehr_preprocess.preprocessors.mimic import MIMIC3Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load procedureevents_mv\n",
    "proc_mv = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', chunksize=100000)\n",
    "# proc_mv.LOCATIONCATEGORY.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(proc_mv):\n",
    "    print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048068657517433167"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get size in gb\n",
    "proc_mv.memory_usage(index=True).sum() / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = pd.read_csv(join(mimic_path, 'INPUTEVENTS_CV.csv.gz'), compression='gzip', nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart.columns[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inmv = pd.read_csv(join(mimic_path, 'INPUTEVENTS_MV.csv.gz'),compression='gzip', nrows=1000,)\n",
    "inmv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(join(processed_path, 'concept.med.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.TIMESTAMP.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TIMESTAMP.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_dic = pd.read_csv(join(mimic_path, 'D_ITEMS.csv.gz'), usecols=['ITEMID', 'LABEL'])\n",
    "input = pd.read_csv(join(mimic_path, 'MICROBIOLOGYEVENTS.csv.gz'), nrows=1000).rename(columns={'SPEC_ITEMID': 'ITEMID'}, inplace=False)\n",
    "input = pd.merge(input, items_dic, on='ITEMID', how='left')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_dic = pd.read_csv(join(mimic_path, 'D_ITEMS.csv.gz'), nrows=10000)\n",
    "# extract labels and itemids as dict\n",
    "items_dic = items_dic[['LABEL', 'ITEMID']].set_index('ITEMID').to_dict()['LABEL']\n",
    "def get_outputevents():\n",
    "    events = pd.read_csv(join(mimic_path, 'OUTPUTEVENTS.csv.gz'), nrows=5000,\n",
    "        usecols=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEOM'])\n",
    "    events['CONCEPT'] = events.ITEMID.map(items_dic)\n",
    "    return events\n",
    "events = get_outputevents()\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(join(mimic_path, 'D_ITEMS.csv.gz'), nrows=10000)\n",
    "items[items.LINKSTO=='outputevents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv(join(mimic_path, 'OUTPUTEVENTS.csv.gz'), nrows=5000)\n",
    "events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'),  compression='gzip',\n",
    "    parse_dates=['ADMITTIME', 'DISCHTIME', 'EDREGTIME', 'EDOUTTIME',],)\n",
    "df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org['adm_diff'] = (df_org['ADMITTIME'] - df_org['EDREGTIME']).dt.days\n",
    "df_org.adm_diff.describe()\n",
    "df_org['adm_diff'] = (df_org['DISCHTIME'] - df_org['EDOUTTIME']).dt.days\n",
    "df_org.adm_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_admission_discharge_to_events( df, start_col, end_col, concept_name):\n",
    "        \"\"\"Convert to events, store as start and end date\"\"\"\n",
    "        discharge = df.copy(deep=True).drop(columns=[start_col])\n",
    "        admission = df.rename(columns={start_col: 'TIMESTAMP'}).drop(columns=[end_col])\n",
    "        admission.drop(columns=['DISCHARGE_LOCATION'], inplace=True)\n",
    "        discharge.drop(columns=['ADMISSION_LOCATION'], inplace=True)\n",
    "        if concept_name=='HOSPITAL':\n",
    "            admission = admission.rename(columns={'ADMISSION_LOCATION': 'VALUE_CAT'})\n",
    "            discharge = discharge.rename(columns={'DISCHARGE_LOCATION': 'VALUE_CAT'})\n",
    "        admission['CONCEPT'] = f'T{concept_name}_ADMISSION'\n",
    "        discharge = discharge.rename(columns={end_col: 'TIMESTAMP'})\n",
    "        discharge['CONCEPT'] = f'T{concept_name}_DISCHARGE'\n",
    "        df = pd.concat([admission, discharge], axis=0)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icu = pd.read_csv(join(mimic_path, 'ICUSTAYS.csv.gz'), compression='gzip',nrows=10000, \n",
    "                parse_dates=['INTIME', 'OUTTIME'])\n",
    "df_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OUTPUTEVENTS\n",
    "df = pd.read_csv(join(mimic_path, 'OUTPUTEVENTS.csv.gz'), nrows=10000,)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=10000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create increasing number where every third number is skipped using range\n",
    "# and list comprehension\n",
    "x = np.array([i for i in range(1, 10000) if i % 10 != 0])\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Items dic\n",
    "items_dic = pd.read_csv(join(mimic_path, 'D_ITEMS.csv.gz'), nrows=10000)\n",
    "items_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# load inputevents cv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(join(mimic_path, \u001b[39m'\u001b[39m\u001b[39mPROCEDUREEVENTS.csv.gz\u001b[39m\u001b[39m'\u001b[39m), nrows\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load inputevents cv\n",
    "df = pd.read_csv(join(mimic_path, 'PROCEDUREEVENTS.csv.gz'), nrows=1000)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'INPUTEVENTS_CV.csv.gz'), compression='gzip',\n",
    "    usecols=['ITEMID'], nrows=int(1e6), dtype={'ITEMID': 'Int32'}, skiprows=np.array([i for i in range(1, int(1e6)) if i % 100 != 0]))\n",
    "df_item_dic= pd.read_csv(join(mimic_path, 'D_ITEMS.csv.gz'), compression='gzip',)\n",
    "df_m = pd.merge(df, df_item_dic[['ITEMID', 'LABEL']], on='ITEMID', how='left')\n",
    "df_m.LABEL.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will only process tables with data not contained in the mimic preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_hdf(join(extr_path, \"all_hourly_data.h5\"), \n",
    "    key='interventions')\n",
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvl = pd.read_hdf(join(extr_path, \"all_hourly_data.h5\"), \n",
    "    key='vitals_labs_mean')\n",
    "dfvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.read_hdf(join(extr_path, \"all_hourly_data.h5\"),\n",
    "    key='patients')\n",
    "dfp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRG codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drg = pd.read_csv(join(mimic_path, 'DRGCODES.csv.gz'),  compression='gzip')\n",
    "df_drg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe relevant, check overlap with ICD codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro = pd.read_csv(join(mimic_path, 'PROCEDURES_ICD.csv.gz'),  compression='gzip')\n",
    "df_pro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time of the procedures not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ce = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=50000)\n",
    "print('RESULTSTATUS',df_ce.RESULTSTATUS.unique(), 'STOPPED', df_ce.STOPPED.unique(), 'WARNING',df_ce.WARNING.unique())\n",
    "print('STORETIME can be dropped, because its the time of entering the data into the database, CHARTTIME is the time of the measurement')\n",
    "print('CGID can be dropped as we don\\'t care about the care giver')\n",
    "print('VALUENUM can be dropped, because we have the VALUE column which containes the full information')\n",
    "print('We can mask out on the ERROR column and drop the ERROR column')\n",
    "print('ROW_ID does not carry any information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=1).columns\n",
    "drop_columns = ['RESULTSTATUS', 'STOPPED','WARNING', 'STORETIME', 'CGID', 'VALUENUM', 'ROW_ID']\n",
    "load_columns = [c for c in all_columns if not c in drop_columns]\n",
    "df_ce = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=int(1e6), usecols=load_columns, parse_dates=['CHARTTIME'])\n",
    "# df_ce = df_ce[df_ce['ERROR'] == 0]\n",
    "# df_ce = df_ce.drop(columns=['ERROR'])\n",
    "print(sys.getsizeof(df_ce)/1e6, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path, columns_str=None, columns=None):\n",
    "    if path.endswith('.gz'):\n",
    "        compression = 'gzip'\n",
    "    else:\n",
    "        compression = None\n",
    "    if not columns_str is None:\n",
    "        all_columns = pl.read_csv(path, n_rows=1, low_memory=True).columns\n",
    "        selected_columns = [c for c in all_columns for s in columns_str if columns_str in c]\n",
    "        if not columns is None:\n",
    "            columns = list(set(columns + selected_columns))\n",
    "        else:\n",
    "            columns = selected_columns\n",
    "    df = pl.read_csv(path, columns=columns, low_memory=True, parse_dates=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_get_columns(path):\n",
    "    if path.endswith('.gz'):\n",
    "        compression = 'gzip'\n",
    "    return pd.read_csv(path, nrows=1, compression=compression).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dest_path_for_parquet(dest_dir, file_path):\n",
    "    file_name = split(file_path)[1]\n",
    "    if not dest_dir is None:\n",
    "        dest_path = join(dest_dir, file_name.replace('.csv.gz', '.parquet.gz'))\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "    else:\n",
    "        file_name = file_name.replace('.csv.gz', '.parquet.gz')\n",
    "        mimic_dir = split(file_path)[0]\n",
    "        mimic_dir_name = split(mimic_dir)[1]\n",
    "        data_dir = split(split(mimic_dir)[0])[0]\n",
    "        interim_mimic_dir = join(data_dir, 'interim', mimic_dir_name)\n",
    "        if not os.path.exists(interim_mimic_dir):\n",
    "            os.makedirs(interim_mimic_dir)\n",
    "        dest_path = join(interim_mimic_dir, file_name)\n",
    "    return dest_path\n",
    "\n",
    "def convert_csv_to_parquet(mimic_path, file_name, dest_dir=None, test=False, nrows=None):\n",
    "    \"\"\"\n",
    "        test: load only 1000 rows\n",
    "        file_name: path of the csv file, with ending\n",
    "        dest_dir: directory to store the parquet files, without ending\n",
    "    \"\"\"\n",
    "    mimic_path = mimic_path# cfg.raw_data_path\n",
    "    dest_dir = dest_dir # cfg.data_working_path\n",
    "    file_path = join(mimic_path, file_name)\n",
    "    columns = pandas_get_columns(file_path)\n",
    "    dtype_dic = {column:\"Int64\" for column in columns if column.endswith('ID') and column!='FLUID'}\n",
    "    if test:\n",
    "        nrows = 300\n",
    "    dest_path = get_dest_path_for_parquet(dest_dir, file_path)\n",
    "    pd.read_csv(file_path, nrows=nrows, dtype=dtype_dic, parse_dates=True, compression='gzip').to_parquet(dest_path, compression='gzip', index=False,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_CV and _MV endings indicate the system used to record the data.\\\n",
    "D prefix is a dictionary table and provides definitions for clinical identifiers.\n",
    "5 tables to track patients: Admissions, Patients, ICUstays, Services, Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group dfs\n",
    "files = os.listdir(mimic_path)\n",
    "tracking_table_names = ['ADMISSIONS', 'PATIENTS', 'ICDUSTAYS', 'SERVICES', 'TRANSFERS']\n",
    "tracking_sheet = [file for file in files for table in tracking_table_names if file.startswith(table)]\n",
    "events_table_names = ['CALLOUT', 'CAREGIVERS', 'CHARTEVENTS', 'CPTEVENTS', 'DATETIMEEVENTS', 'DIAGNOSES_ICD', 'DRGCODES', 'INPUTEVENTS', 'LABEVENTS', 'NOTEEVENTS', 'OUTPUTEVENTS', 'PRESCRIPTIONS', 'PROCEDUREEVENTS', 'PROCEDURES_ICD']\n",
    "events_sheet = [file for file in files for table in events_table_names if file.startswith(table)]\n",
    "dictionary_sheet = [table for table in files if table.startswith('D_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_sheet[0].strip('.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = tracking_sheet + events_sheet + dictionary_sheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.read_csv(join(mimic_path, 'PATIENTS.csv.gz'))\n",
    "dfp[~dfp.DOD.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa[~dfa.DEATHTIME.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICPreprocessor_transfer(MIMIC3Preprocessor):\n",
    "    def __init__(self, cfg, test=False):\n",
    "        super(MIMICPreprocessor_transfer, self).__init__(cfg, test)\n",
    "        self.concept_name = 'transfers'\n",
    "    \n",
    "    def __call__(self):\n",
    "        df = self.load()\n",
    "        df_hospital = self.get_concepts(df, 'ADMITTIME', 'DISCHTIME', 'HOSPITAL')\n",
    "        df_emergency = self.get_concepts(df, 'EDREGTIME', 'EDOUTTIME', 'EMERGENCY')\n",
    "        print(df_hospital)\n",
    "        print(df_emergency)\n",
    "\n",
    "    def load(self):\n",
    "        df = pd.read_csv(join(self.raw_data_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "            usecols=['SUBJECT_ID', 'HADM_ID','ADMITTIME', 'DISCHTIME','DEATHTIME',\n",
    "                'ADMISSION_LOCATION', 'DISCHARGE_LOCATION',\n",
    "                'EDREGTIME','EDOUTTIME'], \n",
    "            parse_dates=['ADMITTIME', 'DISCHTIME', 'EDREGTIME','EDOUTTIME', 'DEATHTIME'])\n",
    "        return df\n",
    "\n",
    "    def get_length_of_stay(self, df, start_col, end_col):\n",
    "        \"\"\"Get length of stay in days, or days until death, store as value\"\"\"\n",
    "        df['VALUE'] = (df[end_col] - df[start_col]).dt.days\n",
    "        mask = df.VALUE.isnull()\n",
    "        df.loc[mask, 'VALUE'] = (df.loc[mask, 'DEATHDATE'] - df.loc[mask, start_col]).dt.days\n",
    "        return df\n",
    "\n",
    "    def convert_admission_discharge_to_events(self, df, start_col, end_col, concept_name):\n",
    "        \"\"\"Convert to events, store as start and end date\"\"\"\n",
    "        dfdis = df.copy(deep=True).drop(columns=[start_col])\n",
    "        df = df.rename(columns={start_col: 'TIMESTAMP'}).drop(columns=[end_col])\n",
    "        if concept_name=='HOSPITAL':\n",
    "            df['VALUE_CAT'] = df['ADMISSION_LOCATION']\n",
    "            dfdis['VALUE_CAT'] = df['DISCHARGE_LOCATION']\n",
    "        df['CONCEPT'] = f'T{concept_name}_ADMISSION'\n",
    "        dfdis = dfdis.rename(columns={end_col: 'TIMESTAMP'})\n",
    "        dfdis['CONCEPT'] = f'T{concept_name}_DISCHARGE'\n",
    "        df = pd.concat([df, dfdis], axis=0)\n",
    "        return df\n",
    "\n",
    "    def get_concepts(self, df, start_col, end_col, concept_name):\n",
    "        \"\"\"Get concepts for admission and discharge, return in standard format\"\"\"\n",
    "        df = df.loc[:, ['SUBJECT_ID', start_col, end_col, 'ADMISSION_TYPE', 'DOD']]\n",
    "        df = self.get_length_of_stay(df, start_col, end_col)\n",
    "        df = self.convert_admission_discharge_to_events(df, start_col, end_col, concept_name)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "    parse_dates=['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'EDREGTIME', 'EDOUTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfa = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "    parse_dates=['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'EDREGTIME', 'EDOUTTIME'])\n",
    "# dfe = dfa[dfa['ADMISSION_TYPE']=='EMERGENCY']\n",
    "# dfe[['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'EDREGTIME', 'EDOUTTIME']]\n",
    "# dfe.loc[dfe.ADMITTIME<dfe.EDREGTIME, ['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'EDREGTIME', 'EDOUTTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "     parse_dates=['ADMITTIME', 'DISCHTIME'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata(concept_name, coding_sys, files_ls):\n",
    "    print('concept_name: ', concept_name)\n",
    "    print('coding_sys: ', coding_sys)\n",
    "    print('files_ls: ', files_ls)\n",
    "metadata_dic ={\n",
    "            'diag': ['ICD9', ['DIAGNOSES_ICD.csv.gz', 'ADMISSIONS.csv.gz']],\n",
    "            'med':['DrugName', ['PRESCRIPTIONS.csv.gz']]\n",
    "\n",
    "        }\n",
    "update_metadata('diag',*metadata_dic['diag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICPreprocessor_transfer:\n",
    "    def __init__(self, test):\n",
    "        print(test)\n",
    "print(globals().keys())\n",
    "class_ = globals()[\"MIMICPreprocessor_transfer\"] \n",
    "#class_ = getattr(globals(), \"MIMICPreprocessor_transfer\")\n",
    "instance = class_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "    parse_dates=['ADMITTIME', 'DISCHTIME'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', \n",
    "    usecols=['SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'ADMISSION_TYPE', 'EDREGTIME','EDOUTTIME'], parse_dates=['ADMITTIME', 'DISCHTIME'])\n",
    "dfp = pd.read_csv(join(mimic_path, 'PATIENTS.csv.gz'), compression='gzip',\n",
    "    usecols=['SUBJECT_ID', 'DOD'], parse_dates=['DOD'])\n",
    "df = df.merge(dfp, on='SUBJECT_ID', how='left')\n",
    "\n",
    "dfa = df.loc[:, ['SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'ADMISSION_TYPE']]\n",
    "# use ADMITTIME and DISCHTIME as separate events to have a time series of events like hospitalization and discharge\n",
    "dfa['VALUE'] = (dfa['DISCHTIME'] - dfa['ADMITTIME']).dt.days\n",
    "dfd = dfa.copy(deep=True).drop(columns=['ADMITTIME'])\n",
    "dfa = dfa.rename(columns={'ADMITTIME': 'TIME'}).drop(columns=['DISCHTIME'])\n",
    "dfa['CONCEPT'] = 'THOSPITAL_ADMISSION'\n",
    "dfd = dfd.rename(columns={'DISCHTIME': 'TIME'})\n",
    "dfd['CONCEPT'] = 'THOSPITAL_DISCHARGE'\n",
    "df = pd.concat([dfa, dfd], axis=0)\n",
    "df.rename(columns={'ADMISSION_TYPE': 'VALUE_CAT'}, inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(mimic_path, 'PROCEDURES_ICD.csv.gz'), compression='gzip', nrows=10000,\n",
    "    )\n",
    "# df.NDC = df.NDC.astype('str')\n",
    "# df['NDC'] = df.NDC.map(lambda x: x[1:])\n",
    "# dfa = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip', nrows=10000, parse_dates=['ADMITTIME'],\n",
    "    # usecols=['HADM_ID', 'ADMITTIME'])\n",
    "# dfh = pd.read_csv(join(data_path, 'helper', 'NDC.csv'), usecols=['ndcpackagecode'])\n",
    "# dfh['ndcpackagecode_simple'] = dfh.ndcpackagecode.str.replace('-', '')\n",
    "# turn into dict\n",
    "# dic = dfh.set_index('ndcpackagecode_simple').to_dict()['ndcpackagecode']\n",
    "# dic[''] = ''\n",
    "#df['NDC'] = df.NDC.map(dic)\n",
    "#df['len'] = df.ndcpackagecode.map(lambda x: len(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.TIMESTAMP.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.CONCEPT=='Furosemide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df[df.VALUE.str.contains('-')].copy()\n",
    "df_cat['VALUE_CAT'] = df_cat['VALUE']\n",
    "df_cat['VALUE'] = df_cat.groupby('CONCEPT')['VALUE'].transform(lambda x: x.astype('category').cat.codes)\n",
    "df_cat.drop(columns=['VALUENUM'], inplace=True)\n",
    "df_cat['VALUE_UNIT'] = 'categorical'\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd['TIMESTAMP'] = dfd['HADM_ID'].map(adm_dic)\n",
    "dfd.rename(columns={'SUBJECT_ID':'PID',  'HADM_ID':'ADMISSION_ID', 'ICD9_CODE':'CONCEPT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd['CONCEPT'] = dfd['CONCEPT'].map(lambda x: 'D'+str(x))\n",
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = pd.read_csv(join(mimic_path, 'LABEVENTS.csv.gz'), compression='gzip', \n",
    "            nrows=30000, \n",
    "            parse_dates=['CHARTTIME'], dtype={'SUBJECT_ID': 'Int32', 'ITEMID': 'Int32', 'VALUE': 'str', 'VALUENUM': 'float32', 'VALUEUOM': 'str', 'HADM_ID': 'Int32'})\n",
    "dfl = dfl.rename(columns={'SUBJECT_ID': 'PID', 'CHARTTIME': 'TIMESTAMP', 'VALUEUOM': 'VALUE_UNIT', 'HADM_ID': 'ADMISSION_ID'}).drop(columns=['ROW_ID', 'FLAG'])\n",
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfld = pd.read_csv(join(mimic_path, 'D_LABITEMS.csv.gz'), compression='gzip')\n",
    "dfld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfld.loc[dfld['LABEL'].str.contains('SPECIMEN'), 'LABEL'] = dfld.loc[dfld['LABEL'].str.contains('SPECIMEN'), 'FLUID'] + ' ' + dfld.loc[dfld['LABEL'].str.contains('SPECIMEN'), 'CATEGORY']\n",
    "dfld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_code_dic = pd.Series(dfld.LOINC_CODE.values, index=dfld.ITEMID).to_dict()\n",
    "item_name_dic = pd.Series(dfld[dfld.LOINC_CODE.isna()].LABEL.values, index=dfld[dfld.LOINC_CODE.isna()].ITEMID).to_dict()\n",
    "# combine dicts\n",
    "item_dic = {**item_code_dic, **item_name_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl['CONCEPT'] = dfl.ITEMID.map(item_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = dfl[dfl['VALUENUM'].notnull()]\n",
    "df_cont = df_cont[df_cont['VALUENUM'] >= 0]\n",
    "df_cont.drop(columns=['VALUE'], inplace=True)\n",
    "df_cont.rename(columns={'VALUENUM': 'VALUE'}, inplace=True)\n",
    "df_cont['VALUE_CAT'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_cont, df_cat], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = dfl[dfl['VALUENUM'].isnull()]\n",
    "# how to avoid the warning :A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead\n",
    "# for this line\n",
    "df_cat.loc[:,'VALUE_CAT'] = df_cat['VALUE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['VALUE'] = df_cat.groupby('CONCEPT')['VALUE'].transform(lambda x: x.astype('category').cat.codes)\n",
    "df_cat.drop(columns=['VALUENUM'], inplace=True)\n",
    "df_cat['VALUE_UNIT'] = 'categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat.rename(columns={'VALUE': 'VALUE_CAT'})\n",
    "df_cont['VALUE_CAT'] = pd.Series(np.nan, index=df_cont.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.concept = df_cat.concept.map(lambda x: 'L'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat[df_cat.concept=='LIntubated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.attrs['SYSTEM'] = 'LOINC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_tab = pa.Table.from_pandas(df_cat)\n",
    "pa_tab.write_metadata = {'SYSTEM': 'LOINC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt.VALUEUOM.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_csv(join(mimic_path, 'D_LABITEMS.csv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.read_csv(join(mimic_path, tracking_sheet[0]), compression='gzip')\n",
    "df_adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.read_csv(join(mimic_path, 'ADMISSIONS.csv.gz'), compression='gzip')\n",
    "print('length', len(df_adm))\n",
    "print('unique patients', len(df_adm['SUBJECT_ID'].unique()))\n",
    "df_adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co = pd.read_csv(join(mimic_path, 'CALLOUT.csv.gz'), compression='gzip')\n",
    "df_co.head()\n",
    "print('length', len(df_co))\n",
    "print('unique patients', len(df_co['SUBJECT_ID'].unique()))\n",
    "df_co.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co = ps.read_csv(join(mimic_path, 'CALLOUT.csv.gz'))\n",
    "sys.getsizeof(df_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not relevant for our analysis\n",
    "df_cg = pd.read_csv(join(mimic_path, 'CAREGIVERS.csv.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=50000)\n",
    "#pd_df.to_parquet(join(mimic_path, 'CHARTEVENTS_sample.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ps = pl.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'),  n_rows=5000, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no information on type of event, only value and ID\n",
    "df_ce = pd.read_csv(join(mimic_path, 'CHARTEVENTS.csv.gz'), compression='gzip', nrows=10000)\n",
    "df_ce.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General approach:\n",
    "- createa dataframe which contains: event_name, timestamp, value, visit, age \n",
    "- separate dataframes into batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ba3af280f6e464a8e38b44b66be462ab3e6654a4c01fb098f7aee24059c7c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
